{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "J0PTBJoFG9gz",
        "7YFzOWRaHAsA",
        "9pQ_dG-3HDRp"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Instructor Embedding lab\n",
        "---\n",
        "This notebook to perform model training and evaluation on 3 Benchmark MTEB, Billboard and Promt Retrieval, with 70 tasks.\n",
        "\n",
        "Just\n",
        "Run all cell below"
      ],
      "metadata": {
        "id": "VFRK-qWqG2-0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup enviroment"
      ],
      "metadata": {
        "id": "J0PTBJoFG9gz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bs8vdqb3oM6-",
        "outputId": "95f845ad-8ca7-4b01-98ea-4984edab3da2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'instructor-embedding'...\n",
            "remote: Enumerating objects: 3091, done.\u001b[K\n",
            "remote: Counting objects: 100% (110/110), done.\u001b[K\n",
            "remote: Compressing objects: 100% (40/40), done.\u001b[K\n",
            "remote: Total 3091 (delta 79), reused 76 (delta 69), pack-reused 2981\u001b[K\n",
            "Receiving objects: 100% (3091/3091), 173.98 MiB | 17.95 MiB/s, done.\n",
            "Resolving deltas: 100% (908/908), done.\n",
            "Updating files: 100% (191/191), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/4ursmile/instructor-embedding"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd instructor-embedding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNHLWYr0oSOW",
        "outputId": "54fd4584-b3ac-4060-f4f0-c480b6c408dd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/instructor-embedding\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UE0mPHROoTuF",
        "outputId": "7564221f-1fdc-4e1e-cd33-2609b78c5418"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.20.0 (from -r requirements.txt (line 1))\n",
            "  Downloading transformers-4.20.0-py3-none-any.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets>=2.2.0 (from -r requirements.txt (line 2))\n",
            "  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyarrow==8.0.0 (from -r requirements.txt (line 3))\n",
            "  Downloading pyarrow-8.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.4/29.4 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonlines (from -r requirements.txt (line 4))\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (1.23.5)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (2.31.0)\n",
            "Requirement already satisfied: scikit_learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (1.11.4)\n",
            "Collecting sentence_transformers>=2.2.0 (from -r requirements.txt (line 9))\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (2.1.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (4.66.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (13.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.20.0->-r requirements.txt (line 1)) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.20.0->-r requirements.txt (line 1)) (0.19.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.20.0->-r requirements.txt (line 1)) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.20.0->-r requirements.txt (line 1)) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.20.0->-r requirements.txt (line 1)) (2023.6.3)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1 (from transformers==4.20.0->-r requirements.txt (line 1))\n",
            "  Downloading tokenizers-0.12.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyarrow-hotfix (from datasets>=2.2.0->-r requirements.txt (line 2))\n",
            "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets>=2.2.0->-r requirements.txt (line 2))\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.2.0->-r requirements.txt (line 2)) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.2.0->-r requirements.txt (line 2)) (3.4.1)\n",
            "Collecting multiprocess (from datasets>=2.2.0->-r requirements.txt (line 2))\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.2.0->-r requirements.txt (line 2)) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.2.0->-r requirements.txt (line 2)) (3.9.1)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines->-r requirements.txt (line 4)) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->-r requirements.txt (line 6)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->-r requirements.txt (line 6)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->-r requirements.txt (line 6)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->-r requirements.txt (line 6)) (2023.11.17)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit_learn>=1.0.2->-r requirements.txt (line 7)) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit_learn>=1.0.2->-r requirements.txt (line 7)) (3.2.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence_transformers>=2.2.0->-r requirements.txt (line 9)) (0.16.0+cu121)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence_transformers>=2.2.0->-r requirements.txt (line 9)) (3.8.1)\n",
            "Collecting sentencepiece (from sentence_transformers>=2.2.0->-r requirements.txt (line 9))\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 10)) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 10)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 10)) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 10)) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 10)) (2.1.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->-r requirements.txt (line 12)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->-r requirements.txt (line 12)) (2.16.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.2.0->-r requirements.txt (line 2)) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.2.0->-r requirements.txt (line 2)) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.2.0->-r requirements.txt (line 2)) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.2.0->-r requirements.txt (line 2)) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.2.0->-r requirements.txt (line 2)) (4.0.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->-r requirements.txt (line 12)) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r requirements.txt (line 10)) (2.1.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers>=2.2.0->-r requirements.txt (line 9)) (8.1.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.2.0->-r requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.2.0->-r requirements.txt (line 2)) (2023.3.post1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->-r requirements.txt (line 10)) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence_transformers>=2.2.0->-r requirements.txt (line 9)) (9.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets>=2.2.0->-r requirements.txt (line 2)) (1.16.0)\n",
            "Building wheels for collected packages: sentence_transformers\n",
            "  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=fd12dc629687d11aad0e35b9f68aea8d4903e0195f8048eb83117a846963afd8\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence_transformers\n",
            "Installing collected packages: tokenizers, sentencepiece, pyarrow-hotfix, pyarrow, jsonlines, dill, multiprocess, transformers, sentence_transformers, datasets\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.15.0\n",
            "    Uninstalling tokenizers-0.15.0:\n",
            "      Successfully uninstalled tokenizers-0.15.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 10.0.1\n",
            "    Uninstalling pyarrow-10.0.1:\n",
            "      Successfully uninstalled pyarrow-10.0.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.35.2\n",
            "    Uninstalling transformers-4.35.2:\n",
            "      Successfully uninstalled transformers-4.35.2\n",
            "Successfully installed datasets-2.15.0 dill-0.3.7 jsonlines-4.0.0 multiprocess-0.70.15 pyarrow-8.0.0 pyarrow-hotfix-0.6 sentence_transformers-2.2.2 sentencepiece-0.1.99 tokenizers-0.12.1 transformers-4.20.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install InstructorEmbedding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0R5URQqoZgx",
        "outputId": "2e7df2b1-4965-44d4-a91d-b9de6873239d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting InstructorEmbedding\n",
            "  Downloading InstructorEmbedding-1.0.1-py2.py3-none-any.whl (19 kB)\n",
            "Installing collected packages: InstructorEmbedding\n",
            "Successfully installed InstructorEmbedding-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Collect data"
      ],
      "metadata": {
        "id": "7YFzOWRaHAsA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown"
      ],
      "metadata": {
        "id": "7ePfZKMgaQjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1vZ5c2oJNonGOvXzppNg5mHz24O6jcc52 -O medi-data.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QY_9ZzVboc3N",
        "outputId": "2dfa803d-d02b-4230-95e6-c9ed4f0bfb51"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1vZ5c2oJNonGOvXzppNg5mHz24O6jcc52\n",
            "To: /content/instructor-embedding/medi-data.zip\n",
            "100% 796M/796M [00:03<00:00, 210MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ./data\n",
        "!unzip medi-data.zip -d ./data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezJWc2M2oeoS",
        "outputId": "b5adab69-8851-402f-bc3a-7b2613002e56"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  medi-data.zip\n",
            "   creating: ./data/medi-data/\n",
            "  inflating: ./data/medi-data/.DS_Store  \n",
            "  inflating: ./data/__MACOSX/medi-data/._.DS_Store  \n",
            "  inflating: ./data/medi-data/medi-data.json  \n",
            "  inflating: ./data/__MACOSX/medi-data/._medi-data.json  \n",
            "  inflating: ./data/medi-data/README.txt  \n",
            "  inflating: ./data/__MACOSX/medi-data/._README.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir logs"
      ],
      "metadata": {
        "id": "Fq_j0yMzogFf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://huggingface.co/sentence-transformers/gtr-t5-large"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxtnxRAuoh4A",
        "outputId": "5738d3fd-2d22-46c3-b37d-1fe3bc402240"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'gtr-t5-large'...\n",
            "remote: Enumerating objects: 20, done.\u001b[K\n",
            "remote: Total 20 (delta 0), reused 0 (delta 0), pack-reused 20\u001b[K\n",
            "Unpacking objects: 100% (20/20), 556.78 KiB | 7.84 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://huggingface.co/sentence-transformers/sentence-t5-large"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyaNyGoRokMe",
        "outputId": "d11bb0fb-81f9-46eb-f035-a9d1f1f690e0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'sentence-t5-large'...\n",
            "remote: Enumerating objects: 20, done.\u001b[K\n",
            "remote: Total 20 (delta 0), reused 0 (delta 0), pack-reused 20\u001b[K\n",
            "Unpacking objects: 100% (20/20), 556.80 KiB | 5.99 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train model"
      ],
      "metadata": {
        "id": "9pQ_dG-3HDRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --model_name_or_path sentence-transformers/gtr-t5-large --output_dir logs --cache_dir data/medi-data --max_source_length 512 --num_train_epochs 10 --save_steps 500 --cl_temperature 0.1 --warmup_ratio 0.1 --learning_rate 2e-5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExpgjceHol0v",
        "outputId": "c00c6d7b-3c98-4128-c0da-6200d7e4305d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-12-19 12:46:24.163589: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-19 12:46:24.163643: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-19 12:46:24.164948: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-19 12:46:24.173159: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-12-19 12:46:25.285622: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Downloading: 100% 1.88k/1.88k [00:00<00:00, 10.2MB/s]\n",
            "Downloading: 100% 773k/773k [00:00<00:00, 10.9MB/s]\n",
            "Downloading: 100% 1.32M/1.32M [00:00<00:00, 20.8MB/s]\n",
            "Downloading: 100% 1.74k/1.74k [00:00<00:00, 10.2MB/s]\n",
            "There are 1435000 pairs to train in total.\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --model_name_or_path sentence-transformers/sentence-t5-large --output_dir logs --cache_dir data/medi-data --max_source_length 512 --num_train_epochs 10 --save_steps 500 --cl_temperature 0.1 --warmup_ratio 0.1 --learning_rate 2e-5"
      ],
      "metadata": {
        "id": "Hd1Niq_CHHt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "isudPRbRHkHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation\n",
        "For evaluation, i added several bash script so, you can easily perform evaluation over 3 benchmark with 1 syntax. Just simply run all cell below"
      ],
      "metadata": {
        "id": "NWjUVPWRL3q6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "H720qndXa4jI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### METB\n",
        "MTEB benchmark with save it result in '/content/instructor-embedding/evaluation/MTEB/results'"
      ],
      "metadata": {
        "id": "9-ZldHedNCGF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Setup"
      ],
      "metadata": {
        "id": "Z9AWHH8VXuZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 777 eval_setup.sh\n",
        "!chmod 777 eval.sh"
      ],
      "metadata": {
        "id": "dYXlSg2QL_v4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./eval_setup.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9GwwV3VL6Zu",
        "outputId": "9db3385c-4c0b-4c19-af42-d606ca66b411"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/instructor-embedding/evaluation/MTEB\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: datasets>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from mteb==1.0.0) (2.15.0)\n",
            "Requirement already satisfied: pyarrow==8.0.0 in /usr/local/lib/python3.10/dist-packages (from mteb==1.0.0) (8.0.0)\n",
            "Requirement already satisfied: jsonlines in /usr/local/lib/python3.10/dist-packages (from mteb==1.0.0) (4.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mteb==1.0.0) (1.23.5)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from mteb==1.0.0) (2.31.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from mteb==1.0.0) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from mteb==1.0.0) (1.11.4)\n",
            "Requirement already satisfied: sentence-transformers>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from mteb==1.0.0) (2.2.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from mteb==1.0.0) (2.1.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from mteb==1.0.0) (4.66.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from mteb==1.0.0) (13.7.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.2.0->mteb==1.0.0) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.2.0->mteb==1.0.0) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.2.0->mteb==1.0.0) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.2.0->mteb==1.0.0) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets>=2.2.0->mteb==1.0.0) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.2.0->mteb==1.0.0) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.2.0->mteb==1.0.0) (3.9.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.2.0->mteb==1.0.0) (0.19.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets>=2.2.0->mteb==1.0.0) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.2.0->mteb==1.0.0) (6.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->mteb==1.0.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->mteb==1.0.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->mteb==1.0.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->mteb==1.0.0) (2023.11.17)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->mteb==1.0.0) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->mteb==1.0.0) (3.2.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.2.0->mteb==1.0.0) (4.20.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.2.0->mteb==1.0.0) (0.16.0+cu121)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.2.0->mteb==1.0.0) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.2.0->mteb==1.0.0) (0.1.99)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->mteb==1.0.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->mteb==1.0.0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->mteb==1.0.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->mteb==1.0.0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->mteb==1.0.0) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->mteb==1.0.0) (2.1.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines->mteb==1.0.0) (23.1.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->mteb==1.0.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->mteb==1.0.0) (2.16.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.2.0->mteb==1.0.0) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.2.0->mteb==1.0.0) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.2.0->mteb==1.0.0) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.2.0->mteb==1.0.0) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.2.0->mteb==1.0.0) (4.0.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->mteb==1.0.0) (0.1.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=2.2.0->mteb==1.0.0) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=2.2.0->mteb==1.0.0) (0.12.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->mteb==1.0.0) (2.1.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers>=2.2.0->mteb==1.0.0) (8.1.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.2.0->mteb==1.0.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.2.0->mteb==1.0.0) (2023.3.post1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->mteb==1.0.0) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers>=2.2.0->mteb==1.0.0) (9.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets>=2.2.0->mteb==1.0.0) (1.16.0)\n",
            "Building wheels for collected packages: mteb\n",
            "  Building editable for mteb (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mteb: filename=mteb-1.0.0-0.editable-py3-none-any.whl size=3199 sha256=817546e020472a400508bfe3e71ea07a0600f7384c310dbc887bc7e183753ba7\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-31fh74n5/wheels/b9/d9/59/b97066182e980e3a97faaac6316214da107c8b35f19460fb74\n",
            "Successfully built mteb\n",
            "Installing collected packages: mteb\n",
            "Successfully installed mteb-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Run Eval"
      ],
      "metadata": {
        "id": "Qmjc2tP2Xw1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TASK_LIST_CLASSIFICATION = [\n",
        "    \"AmazonCounterfactualClassification\",\n",
        "    \"AmazonPolarityClassification\",\n",
        "    \"AmazonReviewsClassification\",\n",
        "    \"Banking77Classification\",\n",
        "    \"EmotionClassification\",\n",
        "    \"ImdbClassification\",\n",
        "    \"MassiveIntentClassification\",\n",
        "    \"MassiveScenarioClassification\",\n",
        "    \"MTOPDomainClassification\",\n",
        "    \"MTOPIntentClassification\",\n",
        "    \"ToxicConversationsClassification\",\n",
        "    \"TweetSentimentExtractionClassification\",\n",
        "]\n",
        "\n",
        "TASK_LIST_CLUSTERING = [\n",
        "    \"ArxivClusteringP2P\",\n",
        "    \"ArxivClusteringS2S\",\n",
        "    \"BiorxivClusteringP2P\",\n",
        "    \"BiorxivClusteringS2S\",\n",
        "    \"MedrxivClusteringP2P\",\n",
        "    \"MedrxivClusteringS2S\",\n",
        "    \"RedditClustering\",\n",
        "    \"RedditClusteringP2P\",\n",
        "    \"StackExchangeClustering\",\n",
        "    \"StackExchangeClusteringP2P\",\n",
        "    \"TwentyNewsgroupsClustering\",\n",
        "]\n",
        "\n",
        "TASK_LIST_PAIR_CLASSIFICATION = [\n",
        "    \"SprintDuplicateQuestions\",\n",
        "    \"TwitterSemEval2015\",\n",
        "    \"TwitterURLCorpus\",\n",
        "]\n",
        "\n",
        "TASK_LIST_RERANKING = [\n",
        "    \"AskUbuntuDupQuestions\",\n",
        "    \"MindSmallReranking\",\n",
        "    \"SciDocsRR\",\n",
        "    \"StackOverflowDupQuestions\",\n",
        "]\n",
        "\n",
        "TASK_LIST_RETRIEVAL = [\n",
        "    \"ArguAna\",\n",
        "    \"ClimateFEVER\",\n",
        "    \"CQADupstackAndroidRetrieval\",\n",
        "    \"CQADupstackEnglishRetrieval\",\n",
        "    \"CQADupstackGamingRetrieval\",\n",
        "    \"CQADupstackGisRetrieval\",\n",
        "    \"CQADupstackMathematicaRetrieval\",\n",
        "    \"CQADupstackPhysicsRetrieval\",\n",
        "    \"CQADupstackProgrammersRetrieval\",\n",
        "    \"CQADupstackStatsRetrieval\",\n",
        "    \"CQADupstackTexRetrieval\",\n",
        "    \"CQADupstackUnixRetrieval\",\n",
        "    \"CQADupstackWebmastersRetrieval\",\n",
        "    \"CQADupstackWordpressRetrieval\",\n",
        "    \"DBPedia\",\n",
        "    \"FEVER\",\n",
        "    \"FiQA2018\",\n",
        "    \"HotpotQA\",\n",
        "    \"MSMARCO\",\n",
        "    \"NFCorpus\",\n",
        "    \"NQ\",\n",
        "    \"QuoraRetrieval\",\n",
        "    \"SCIDOCS\",\n",
        "    \"SciFact\",\n",
        "    \"Touche2020\",\n",
        "    \"TRECCOVID\",\n",
        "]\n",
        "\n",
        "TASK_LIST_STS = [\n",
        "    \"BIOSSES\",\n",
        "    \"SICK-R\",\n",
        "    \"STS12\",\n",
        "    \"STS13\",\n",
        "    \"STS14\",\n",
        "    \"STS15\",\n",
        "    \"STS16\",\n",
        "    \"STS17\",\n",
        "    \"STS22\",\n",
        "    \"STSBenchmark\",\n",
        "    \"SummEval\",\n",
        "]\n",
        "mteb_task_list = [TASK_LIST_CLASSIFICATION, TASK_LIST_CLUSTERING, TASK_LIST_PAIR_CLASSIFICATION, TASK_LIST_RERANKING, TASK_LIST_RETRIEVAL, TASK_LIST_STS]\n",
        "model = 'large'"
      ],
      "metadata": {
        "id": "tfuyJlHqN5Gi"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "for tasks in mteb_task_list:\n",
        "  for task in tasks:\n",
        "    res = subprocess.run([\"./eval.sh\", model, task], capture_output=True, text=True).stdout.strip(\"\\n\")\n",
        "    print(res)\n",
        "    print('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_JSypUfO1sz",
        "outputId": "8480de5a-0bdd-4d78-f78b-7c0402183936"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load INSTRUCTOR_Transformer\n",
            "max_seq_length  512\n",
            "───────────────────────────────────────── Selected tasks  ──────────────────────────────────────────\n",
            "Classification\n",
            "    - AmazonCounterfactualClassification, s2s, multilingual 4 langs\n",
            "\n",
            "\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "Task: AmazonCounterfactualClassification, split: test, language: en. Running...\n",
            "use logRegClassificationEvaluator\n",
            "with prompt\n",
            "with prompt\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "use logRegClassificationEvaluator\n",
            "with prompt\n",
            "with prompt\n",
            "use logRegClassificationEvaluator\n",
            "with prompt\n",
            "with prompt\n",
            "use logRegClassificationEvaluator\n",
            "with prompt\n",
            "with prompt\n",
            "use logRegClassificationEvaluator\n",
            "with prompt\n",
            "with prompt\n",
            "use logRegClassificationEvaluator\n",
            "with prompt\n",
            "with prompt\n",
            "use logRegClassificationEvaluator\n",
            "with prompt\n",
            "with prompt\n",
            "use logRegClassificationEvaluator\n",
            "with prompt\n",
            "with prompt\n",
            "use logRegClassificationEvaluator\n",
            "with prompt\n",
            "with prompt\n",
            "use logRegClassificationEvaluator\n",
            "with prompt\n",
            "with prompt\n",
            "--DONE--\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Move result to drive"
      ],
      "metadata": {
        "id": "mcFIEkmrYGR5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Billboard"
      ],
      "metadata": {
        "id": "He7iVyV8X0gr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Setup"
      ],
      "metadata": {
        "id": "eGfZGLI9X3nd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 777 eval_billboard_setup.sh\n",
        "!chmod 777 eval_billboard.sh"
      ],
      "metadata": {
        "id": "qwBOc2d5YzYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./eval_billboard_setup.sh"
      ],
      "metadata": {
        "id": "9cqyRvfbZCHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Run eval"
      ],
      "metadata": {
        "id": "a3sVIDVZX5VS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "billboard_task_list = ['mscoco', 'cnn summary', 'machine translation']"
      ],
      "metadata": {
        "id": "T4oisaf3ZGm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for task in billboard_task_list:\n",
        "    res = subprocess.run([\"./eval_billboard.sh\", model, task], capture_output=True, text=True).stdout.strip(\"\\n\")\n",
        "    print(res)\n",
        "    print('\\n')"
      ],
      "metadata": {
        "id": "4UQho55gZcns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Move result to drive"
      ],
      "metadata": {
        "id": "bFE-I5d4YQB7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Promt Retrieval"
      ],
      "metadata": {
        "id": "wFTAN0r0X70m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Setup"
      ],
      "metadata": {
        "id": "GSUvuNsyX_J1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 777 eval_promt_retrieval_setup.sh\n",
        "!chmod 777 eval_promt_retrieval.sh"
      ],
      "metadata": {
        "id": "hQiWLkgiZr8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./eval_promt_retrieval_setup.sh"
      ],
      "metadata": {
        "id": "M_HTXW2-ZsOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Run eval"
      ],
      "metadata": {
        "id": "Hor_p7-hYCON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "promt_retrieval_task_list = ['rte','sst5','mrpc','dbpedia_14','hellaswag']"
      ],
      "metadata": {
        "id": "zP0uGXCWZ1LF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for task in promt_retrieval_task_list:\n",
        "    res = subprocess.run([\"./eval_promt_retrieval.sh\", model, task], capture_output=True, text=True).stdout.strip(\"\\n\")\n",
        "    print(res)\n",
        "    print('\\n')"
      ],
      "metadata": {
        "id": "sypZbNLhaDcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Move result to drive"
      ],
      "metadata": {
        "id": "wx9nCxDrYS0Y"
      }
    }
  ]
}