{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFRK-qWqG2-0"
      },
      "source": [
        "# Instructor Embedding lab\n",
        "---\n",
        "This notebook to perform model training and evaluation on 3 Benchmark MTEB, Billboard and Promt Retrieval, with 70 tasks.\n",
        "\n",
        "Just\n",
        "Run all cell below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0PTBJoFG9gz"
      },
      "source": [
        "## Setup enviroment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bs8vdqb3oM6-",
        "outputId": "f80fae0d-b2db-412e-ea2e-c420dae7c929"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'instructor-embedding'...\n",
            "remote: Enumerating objects: 3137, done.\u001b[K\n",
            "remote: Counting objects: 100% (156/156), done.\u001b[K\n",
            "remote: Compressing objects: 100% (75/75), done.\u001b[K\n",
            "remote: Total 3137 (delta 99), reused 112 (delta 80), pack-reused 2981\u001b[K\n",
            "Receiving objects: 100% (3137/3137), 176.80 MiB | 10.09 MiB/s, done.\n",
            "Resolving deltas: 100% (928/928), done.\n",
            "Updating files: 100% (199/199), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/4ursmile/instructor-embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNHLWYr0oSOW",
        "outputId": "f83821bd-382e-4ba2-87fa-be9d3a73d76c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/instructor-embedding\n"
          ]
        }
      ],
      "source": [
        "%cd instructor-embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UE0mPHROoTuF",
        "outputId": "0ad18be7-03a1-41cb-c8ad-f128c504db76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.20.0 (from -r requirements.txt (line 1))\n",
            "  Downloading transformers-4.20.0-py3-none-any.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets>=2.2.0 (from -r requirements.txt (line 2))\n",
            "  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyarrow==8.0.0 (from -r requirements.txt (line 3))\n",
            "  Downloading pyarrow-8.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.4/29.4 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonlines (from -r requirements.txt (line 4))\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (1.23.5)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (2.31.0)\n",
            "Requirement already satisfied: scikit_learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (1.11.4)\n",
            "Collecting sentence_transformers>=2.2.0 (from -r requirements.txt (line 9))\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (2.1.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (4.66.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (13.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.20.0->-r requirements.txt (line 1)) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.20.0->-r requirements.txt (line 1)) (0.19.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.20.0->-r requirements.txt (line 1)) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.20.0->-r requirements.txt (line 1)) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.20.0->-r requirements.txt (line 1)) (2023.6.3)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1 (from transformers==4.20.0->-r requirements.txt (line 1))\n",
            "  Downloading tokenizers-0.12.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m111.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyarrow-hotfix (from datasets>=2.2.0->-r requirements.txt (line 2))\n",
            "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets>=2.2.0->-r requirements.txt (line 2))\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.2.0->-r requirements.txt (line 2)) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.2.0->-r requirements.txt (line 2)) (3.4.1)\n",
            "Collecting multiprocess (from datasets>=2.2.0->-r requirements.txt (line 2))\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.2.0->-r requirements.txt (line 2)) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.2.0->-r requirements.txt (line 2)) (3.9.1)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines->-r requirements.txt (line 4)) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->-r requirements.txt (line 6)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->-r requirements.txt (line 6)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->-r requirements.txt (line 6)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->-r requirements.txt (line 6)) (2023.11.17)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit_learn>=1.0.2->-r requirements.txt (line 7)) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit_learn>=1.0.2->-r requirements.txt (line 7)) (3.2.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence_transformers>=2.2.0->-r requirements.txt (line 9)) (0.16.0+cu121)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence_transformers>=2.2.0->-r requirements.txt (line 9)) (3.8.1)\n",
            "Collecting sentencepiece (from sentence_transformers>=2.2.0->-r requirements.txt (line 9))\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 10)) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 10)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 10)) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 10)) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 10)) (2.1.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->-r requirements.txt (line 12)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->-r requirements.txt (line 12)) (2.16.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.2.0->-r requirements.txt (line 2)) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.2.0->-r requirements.txt (line 2)) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.2.0->-r requirements.txt (line 2)) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.2.0->-r requirements.txt (line 2)) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.2.0->-r requirements.txt (line 2)) (4.0.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->-r requirements.txt (line 12)) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r requirements.txt (line 10)) (2.1.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers>=2.2.0->-r requirements.txt (line 9)) (8.1.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.2.0->-r requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.2.0->-r requirements.txt (line 2)) (2023.3.post1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->-r requirements.txt (line 10)) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence_transformers>=2.2.0->-r requirements.txt (line 9)) (9.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets>=2.2.0->-r requirements.txt (line 2)) (1.16.0)\n",
            "Building wheels for collected packages: sentence_transformers\n",
            "  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=6befd8763ce67aa3f3e026f0cdc6c79cde384c4b4107a14bf1bfa6680b0776d5\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence_transformers\n",
            "Installing collected packages: tokenizers, sentencepiece, pyarrow-hotfix, pyarrow, jsonlines, dill, multiprocess, transformers, sentence_transformers, datasets\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.15.0\n",
            "    Uninstalling tokenizers-0.15.0:\n",
            "      Successfully uninstalled tokenizers-0.15.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 10.0.1\n",
            "    Uninstalling pyarrow-10.0.1:\n",
            "      Successfully uninstalled pyarrow-10.0.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.35.2\n",
            "    Uninstalling transformers-4.35.2:\n",
            "      Successfully uninstalled transformers-4.35.2\n",
            "Successfully installed datasets-2.15.0 dill-0.3.7 jsonlines-4.0.0 multiprocess-0.70.15 pyarrow-8.0.0 pyarrow-hotfix-0.6 sentence_transformers-2.2.2 sentencepiece-0.1.99 tokenizers-0.12.1 transformers-4.20.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0R5URQqoZgx",
        "outputId": "2aececea-8a27-45f3-91f8-08a92805b284"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting InstructorEmbedding\n",
            "  Downloading InstructorEmbedding-1.0.1-py2.py3-none-any.whl (19 kB)\n",
            "Installing collected packages: InstructorEmbedding\n",
            "Successfully installed InstructorEmbedding-1.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install InstructorEmbedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YFzOWRaHAsA"
      },
      "source": [
        "## Collect data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ePfZKMgaQjf",
        "outputId": "e62253fc-f5c5-4fd1-9389-ad0d5f4830fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.6.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.13.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2023.11.17)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QY_9ZzVboc3N",
        "outputId": "ad54d7d1-6f3f-41c1-cb3c-6f5fcc370c55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1vZ5c2oJNonGOvXzppNg5mHz24O6jcc52\n",
            "To: /content/instructor-embedding/medi-data.zip\n",
            "100% 796M/796M [00:21<00:00, 36.5MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown 1vZ5c2oJNonGOvXzppNg5mHz24O6jcc52 -O medi-data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezJWc2M2oeoS",
        "outputId": "0f6c4992-5a16-4117-8b80-26b08108bb13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  medi-data.zip\n",
            "   creating: ./data/medi-data/\n",
            "  inflating: ./data/medi-data/.DS_Store  \n",
            "  inflating: ./data/__MACOSX/medi-data/._.DS_Store  \n",
            "  inflating: ./data/medi-data/medi-data.json  \n",
            "  inflating: ./data/__MACOSX/medi-data/._medi-data.json  \n",
            "  inflating: ./data/medi-data/README.txt  \n",
            "  inflating: ./data/__MACOSX/medi-data/._README.txt  \n"
          ]
        }
      ],
      "source": [
        "!mkdir ./data\n",
        "!unzip medi-data.zip -d ./data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fq_j0yMzogFf"
      },
      "outputs": [],
      "source": [
        "!mkdir logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxtnxRAuoh4A",
        "outputId": "a077ae20-eb38-4f71-bbf9-d2fec6fe53d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'gtr-t5-large'...\n",
            "remote: Enumerating objects: 20, done.\u001b[K\n",
            "remote: Total 20 (delta 0), reused 0 (delta 0), pack-reused 20\u001b[K\n",
            "Unpacking objects: 100% (20/20), 556.78 KiB | 1007.00 KiB/s, done.\n",
            "Filtering content: 100% (3/3), 642.69 MiB | 9.03 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://huggingface.co/sentence-transformers/gtr-t5-large"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyaNyGoRokMe",
        "outputId": "202cb399-12ea-44b2-ca18-38da8c0e30e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'sentence-t5-large'...\n",
            "remote: Enumerating objects: 20, done.\u001b[K\n",
            "remote: Total 20 (delta 0), reused 0 (delta 0), pack-reused 20\u001b[K\n",
            "Unpacking objects: 100% (20/20), 556.80 KiB | 1.76 MiB/s, done.\n",
            "Filtering content: 100% (3/3), 642.69 MiB | 10.06 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://huggingface.co/sentence-transformers/sentence-t5-large"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pQ_dG-3HDRp"
      },
      "source": [
        "## Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ExpgjceHol0v"
      },
      "outputs": [],
      "source": [
        "#!python train.py --model_name_or_path sentence-transformers/gtr-t5-large --output_dir logs --cache_dir data/medi-data --max_source_length 512 --num_train_epochs 10 --save_steps 500 --cl_temperature 0.1 --warmup_ratio 0.1 --learning_rate 2e-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hd1Niq_CHHt7"
      },
      "outputs": [],
      "source": [
        "#!python train.py --model_name_or_path sentence-transformers/sentence-t5-large --output_dir logs --cache_dir data/medi-data --max_source_length 512 --num_train_epochs 10 --save_steps 500 --cl_temperature 0.1 --warmup_ratio 0.1 --learning_rate 2e-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "isudPRbRHkHu"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWjUVPWRL3q6"
      },
      "source": [
        "## Evaluation\n",
        "For evaluation, i added several bash script and evaluation python script to evaluate throught all task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-ZldHedNCGF"
      },
      "source": [
        "### METB\n",
        "MTEB benchmark with save it result in '/content/instructor-embedding/evaluation/MTEB/results'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9AWHH8VXuZL"
      },
      "source": [
        "#### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOGmRC_V0Y-f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "574a0f65-2b00-4df1-feb5-3ef7412aca85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mteb\n",
            "  Downloading mteb-1.1.1-py3-none-any.whl (119 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/119.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.2/119.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: datasets>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from mteb) (2.15.0)\n",
            "Requirement already satisfied: jsonlines in /usr/local/lib/python3.10/dist-packages (from mteb) (4.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mteb) (1.23.5)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from mteb) (2.31.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from mteb) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from mteb) (1.11.4)\n",
            "Requirement already satisfied: sentence-transformers>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from mteb) (2.2.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from mteb) (2.1.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from mteb) (4.66.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from mteb) (13.7.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.2.0->mteb) (8.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.2.0->mteb) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.2.0->mteb) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.2.0->mteb) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.2.0->mteb) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets>=2.2.0->mteb) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.2.0->mteb) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.2.0->mteb) (3.9.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.2.0->mteb) (0.19.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets>=2.2.0->mteb) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.2.0->mteb) (6.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->mteb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->mteb) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->mteb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->mteb) (2023.11.17)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->mteb) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->mteb) (3.2.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.2.0->mteb) (4.20.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.2.0->mteb) (0.16.0+cu121)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.2.0->mteb) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.2.0->mteb) (0.1.99)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->mteb) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->mteb) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->mteb) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->mteb) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->mteb) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->mteb) (2.1.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines->mteb) (23.1.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->mteb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->mteb) (2.16.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.2.0->mteb) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.2.0->mteb) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.2.0->mteb) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.2.0->mteb) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.2.0->mteb) (4.0.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->mteb) (0.1.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=2.2.0->mteb) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=2.2.0->mteb) (0.12.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->mteb) (2.1.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers>=2.2.0->mteb) (8.1.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.2.0->mteb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.2.0->mteb) (2023.3.post1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->mteb) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers>=2.2.0->mteb) (9.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets>=2.2.0->mteb) (1.16.0)\n",
            "Installing collected packages: mteb\n",
            "Successfully installed mteb-1.1.1\n",
            "Requirement already satisfied: mteb[beir] in /usr/local/lib/python3.10/dist-packages (1.1.1)\n",
            "Requirement already satisfied: datasets>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from mteb[beir]) (2.15.0)\n",
            "Requirement already satisfied: jsonlines in /usr/local/lib/python3.10/dist-packages (from mteb[beir]) (4.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mteb[beir]) (1.23.5)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from mteb[beir]) (2.31.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from mteb[beir]) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from mteb[beir]) (1.11.4)\n",
            "Requirement already satisfied: sentence-transformers>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from mteb[beir]) (2.2.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from mteb[beir]) (2.1.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from mteb[beir]) (4.66.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from mteb[beir]) (13.7.0)\n",
            "Collecting beir (from mteb[beir])\n",
            "  Downloading beir-2.0.0.tar.gz (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.2.0->mteb[beir]) (8.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.2.0->mteb[beir]) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.2.0->mteb[beir]) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.2.0->mteb[beir]) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.2.0->mteb[beir]) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets>=2.2.0->mteb[beir]) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.2.0->mteb[beir]) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.2.0->mteb[beir]) (3.9.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.2.0->mteb[beir]) (0.19.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets>=2.2.0->mteb[beir]) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.2.0->mteb[beir]) (6.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->mteb[beir]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->mteb[beir]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->mteb[beir]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->mteb[beir]) (2023.11.17)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->mteb[beir]) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->mteb[beir]) (3.2.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.2.0->mteb[beir]) (4.20.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.2.0->mteb[beir]) (0.16.0+cu121)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.2.0->mteb[beir]) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.2.0->mteb[beir]) (0.1.99)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->mteb[beir]) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->mteb[beir]) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->mteb[beir]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->mteb[beir]) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->mteb[beir]) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->mteb[beir]) (2.1.0)\n",
            "Collecting pytrec_eval (from beir->mteb[beir])\n",
            "  Downloading pytrec_eval-0.5.tar.gz (15 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting faiss_cpu (from beir->mteb[beir])\n",
            "  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting elasticsearch==7.9.1 (from beir->mteb[beir])\n",
            "  Downloading elasticsearch-7.9.1-py2.py3-none-any.whl (219 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.2/219.2 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines->mteb[beir]) (23.1.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->mteb[beir]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->mteb[beir]) (2.16.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.2.0->mteb[beir]) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.2.0->mteb[beir]) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.2.0->mteb[beir]) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.2.0->mteb[beir]) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.2.0->mteb[beir]) (4.0.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->mteb[beir]) (0.1.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=2.2.0->mteb[beir]) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=2.2.0->mteb[beir]) (0.12.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->mteb[beir]) (2.1.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers>=2.2.0->mteb[beir]) (8.1.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.2.0->mteb[beir]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.2.0->mteb[beir]) (2023.3.post1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->mteb[beir]) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers>=2.2.0->mteb[beir]) (9.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets>=2.2.0->mteb[beir]) (1.16.0)\n",
            "Building wheels for collected packages: beir, pytrec_eval\n",
            "  Building wheel for beir (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for beir: filename=beir-2.0.0-py3-none-any.whl size=63550 sha256=e9497b6d372e9cd2ab8eb2a90b8d3d5f97dc0216ed7223d428dbc3922482855c\n",
            "  Stored in directory: /root/.cache/pip/wheels/1c/14/96/c606ede3c10e9300ef771a6183af09d389459195ff5f854862\n",
            "  Building wheel for pytrec_eval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytrec_eval: filename=pytrec_eval-0.5-cp310-cp310-linux_x86_64.whl size=308213 sha256=5679e8894ae58edc0327de5c35beb74181517bb130a52535dcafcaf53540a341\n",
            "  Stored in directory: /root/.cache/pip/wheels/51/3a/cd/dcc1ddfc763987d5cb237165d8ac249aa98a23ab90f67317a8\n",
            "Successfully built beir pytrec_eval\n",
            "Installing collected packages: faiss_cpu, pytrec_eval, elasticsearch, beir\n",
            "Successfully installed beir-2.0.0 elasticsearch-7.9.1 faiss_cpu-1.7.4 pytrec_eval-0.5\n",
            "Collecting openai\n",
            "  Downloading openai-1.6.1-py3-none-any.whl (225 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.4/225.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Collecting typing-extensions<5,>=4.7 (from openai)\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: typing-extensions, h11, httpcore, httpx, openai\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h11-0.14.0 httpcore-1.0.2 httpx-0.26.0 openai-1.6.1 typing-extensions-4.9.0\n",
            "Successfully installed MTEB\n",
            "Successfully created cache folder\n",
            "Successfully created outputs and cache folder\n",
            "Successfully Set up the evaluation folder\n"
          ]
        }
      ],
      "source": [
        "!python eval_setup.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qmjc2tP2Xw1X"
      },
      "source": [
        "#### Run Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfuyJlHqN5Gi"
      },
      "outputs": [],
      "source": [
        "TASK_LIST_CLASSIFICATION = [\n",
        "    \"AmazonCounterfactualClassification\",\n",
        "    \"AmazonPolarityClassification\",\n",
        "    \"AmazonReviewsClassification\",\n",
        "    \"Banking77Classification\",\n",
        "    \"EmotionClassification\",\n",
        "    \"ImdbClassification\",\n",
        "    \"MassiveIntentClassification\",\n",
        "    \"MassiveScenarioClassification\",\n",
        "    \"MTOPDomainClassification\",\n",
        "    \"MTOPIntentClassification\",\n",
        "    \"ToxicConversationsClassification\",\n",
        "    \"TweetSentimentExtractionClassification\",\n",
        "]\n",
        "\n",
        "TASK_LIST_CLUSTERING = [\n",
        "    \"ArxivClusteringP2P\",\n",
        "    \"ArxivClusteringS2S\",\n",
        "    \"BiorxivClusteringP2P\",\n",
        "    \"BiorxivClusteringS2S\",\n",
        "    \"MedrxivClusteringP2P\",\n",
        "    \"MedrxivClusteringS2S\",\n",
        "    \"RedditClustering\",\n",
        "    \"RedditClusteringP2P\",\n",
        "    \"StackExchangeClustering\",\n",
        "    \"StackExchangeClusteringP2P\",\n",
        "    \"TwentyNewsgroupsClustering\",\n",
        "]\n",
        "\n",
        "TASK_LIST_PAIR_CLASSIFICATION = [\n",
        "    \"SprintDuplicateQuestions\",\n",
        "    \"TwitterSemEval2015\",\n",
        "    \"TwitterURLCorpus\",\n",
        "]\n",
        "\n",
        "TASK_LIST_RERANKING = [\n",
        "    \"AskUbuntuDupQuestions\",\n",
        "    \"MindSmallReranking\",\n",
        "    \"SciDocsRR\",\n",
        "    \"StackOverflowDupQuestions\",\n",
        "]\n",
        "\n",
        "TASK_LIST_RETRIEVAL = [\n",
        "    \"ArguAna\",\n",
        "    \"ClimateFEVER\",\n",
        "    \"CQADupstackAndroidRetrieval\",\n",
        "    \"CQADupstackEnglishRetrieval\",\n",
        "    \"CQADupstackGamingRetrieval\",\n",
        "    \"CQADupstackGisRetrieval\",\n",
        "    \"CQADupstackMathematicaRetrieval\",\n",
        "    \"CQADupstackPhysicsRetrieval\",\n",
        "    \"CQADupstackProgrammersRetrieval\",\n",
        "    \"CQADupstackStatsRetrieval\",\n",
        "    \"CQADupstackTexRetrieval\",\n",
        "    \"CQADupstackUnixRetrieval\",\n",
        "    \"CQADupstackWebmastersRetrieval\",\n",
        "    \"CQADupstackWordpressRetrieval\",\n",
        "    \"DBPedia\",\n",
        "    \"FEVER\",\n",
        "    \"FiQA2018\",\n",
        "    \"HotpotQA\",\n",
        "    \"MSMARCO\",\n",
        "    \"NFCorpus\",\n",
        "    \"NQ\",\n",
        "    \"QuoraRetrieval\",\n",
        "    \"SCIDOCS\",\n",
        "    \"SciFact\",\n",
        "    \"Touche2020\",\n",
        "    \"TRECCOVID\",\n",
        "]\n",
        "\n",
        "TASK_LIST_STS = [\n",
        "    \"BIOSSES\",\n",
        "    \"SICK-R\",\n",
        "    \"STS12\",\n",
        "    \"STS13\",\n",
        "    \"STS14\",\n",
        "    \"STS15\",\n",
        "    \"STS16\",\n",
        "    \"STS17\",\n",
        "    \"STS22\",\n",
        "    \"STSBenchmark\",\n",
        "    \"SummEval\",\n",
        "]\n",
        "mteb_task_list = [TASK_LIST_CLASSIFICATION, TASK_LIST_CLUSTERING, TASK_LIST_PAIR_CLASSIFICATION, TASK_LIST_RERANKING, TASK_LIST_RETRIEVAL, TASK_LIST_STS]\n",
        "model = 'large'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_JSypUfO1sz",
        "outputId": "1e02184b-f94b-4016-e9f1-6d63c6bd3bf6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: hkunlp/instructor-large\n",
            "load INSTRUCTOR_Transformer\n",
            "max_seq_length  512\n",
            "INFO:sentence_transformers.SentenceTransformer:Use pytorch device: cuda\n",
            "INFO:mteb.evaluation.MTEB:\n",
            "\n",
            "## Evaluating 1 tasks:\n",
            "\u001b[30m───────────────────────────────────────── \u001b[0m\u001b[1mSelected tasks \u001b[0m\u001b[30m ──────────────────────────────────────────\u001b[0m\n",
            "\u001b[1mClassification\u001b[0m\n",
            "    - AmazonCounterfactualClassification, \u001b[3;90ms2s\u001b[0m, \u001b[3;31mmultilingual \u001b[0m\u001b[1;3;31m1\u001b[0m\u001b[3;31m \u001b[0m\u001b[3;31m/\u001b[0m\u001b[3;31m \u001b[0m\u001b[1;3;31m4\u001b[0m\u001b[3;31m langs\u001b[0m\n",
            "\n",
            "\n",
            "INFO:mteb.evaluation.MTEB:\n",
            "\n",
            "********************** Evaluating AmazonCounterfactualClassification **********************\n",
            "WARNING:mteb.evaluation.MTEB:WARNING: AmazonCounterfactualClassification results already exists. Skipping.\n",
            "--DONE--\n",
            "\n",
            "\n",
            "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: hkunlp/instructor-large\n",
            "load INSTRUCTOR_Transformer\n",
            "max_seq_length  512\n",
            "INFO:sentence_transformers.SentenceTransformer:Use pytorch device: cuda\n",
            "INFO:mteb.evaluation.MTEB:\n",
            "\n",
            "## Evaluating 1 tasks:\n",
            "\u001b[30m───────────────────────────────────────── \u001b[0m\u001b[1mSelected tasks \u001b[0m\u001b[30m ──────────────────────────────────────────\u001b[0m\n",
            "\u001b[1mClassification\u001b[0m\n",
            "    - AmazonPolarityClassification, \u001b[3;90mp2p\u001b[0m\n",
            "\n",
            "\n",
            "INFO:mteb.evaluation.MTEB:\n",
            "\n",
            "********************** Evaluating AmazonPolarityClassification **********************\n",
            "INFO:mteb.evaluation.MTEB:Loading dataset for AmazonPolarityClassification\n",
            "INFO:mteb.abstasks.AbsTaskClassification:\n",
            "Task: AmazonPolarityClassification, split: test. Running...\n",
            "INFO:mteb.abstasks.AbsTaskClassification:========== Experiment 1/10 ==========\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Encoding 16 training sentences...\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Encoding 400000 test sentences...\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Fitting logistic regression classifier...\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Evaluating...\n",
            "INFO:mteb.abstasks.AbsTaskClassification:========== Experiment 2/10 ==========\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Encoding 16 training sentences...\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Encoding 400000 test sentences...\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Fitting logistic regression classifier...\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Evaluating...\n",
            "INFO:mteb.abstasks.AbsTaskClassification:========== Experiment 3/10 ==========\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Encoding 16 training sentences...\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Encoding 400000 test sentences...\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Fitting logistic regression classifier...\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Evaluating...\n",
            "INFO:mteb.abstasks.AbsTaskClassification:========== Experiment 4/10 ==========\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Encoding 16 training sentences...\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Encoding 400000 test sentences...\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Fitting logistic regression classifier...\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Evaluating...\n",
            "INFO:mteb.abstasks.AbsTaskClassification:========== Experiment 5/10 ==========\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Encoding 16 training sentences...\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Encoding 400000 test sentences...\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Fitting logistic regression classifier...\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Evaluating...\n",
            "INFO:mteb.abstasks.AbsTaskClassification:========== Experiment 6/10 ==========\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Encoding 16 training sentences...\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Encoding 400000 test sentences...\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Fitting logistic regression classifier...\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Evaluating...\n",
            "INFO:mteb.abstasks.AbsTaskClassification:========== Experiment 7/10 ==========\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Encoding 16 training sentences...\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Encoding 400000 test sentences...\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Fitting logistic regression classifier...\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Evaluating...\n",
            "INFO:mteb.abstasks.AbsTaskClassification:========== Experiment 8/10 ==========\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Encoding 16 training sentences...\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Encoding 400000 test sentences...\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Fitting logistic regression classifier...\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Evaluating...\n",
            "INFO:mteb.abstasks.AbsTaskClassification:========== Experiment 9/10 ==========\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Encoding 16 training sentences...\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Encoding 400000 test sentences...\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Fitting logistic regression classifier...\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Evaluating...\n",
            "INFO:mteb.abstasks.AbsTaskClassification:========== Experiment 10/10 ==========\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Encoding 16 training sentences...\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Encoding 400000 test sentences...\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Fitting logistic regression classifier...\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Evaluating...\n",
            "INFO:mteb.evaluation.MTEB:Evaluation for AmazonPolarityClassification on test took 12194.66 seconds\n",
            "INFO:mteb.evaluation.MTEB:Scores: {'accuracy': 0.9216524999999999, 'f1': 0.921544082749149, 'ap': 0.8893614250973277, 'accuracy_stderr': 0.014459706990461457, 'f1_stderr': 0.01458484035886379, 'ap_stderr': 0.015434013191625769, 'main_score': 0.9216524999999999, 'evaluation_time': 12194.66}\n",
            "--DONE--\n",
            "\n",
            "\n",
            "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: hkunlp/instructor-large\n",
            "load INSTRUCTOR_Transformer\n",
            "max_seq_length  512\n",
            "INFO:sentence_transformers.SentenceTransformer:Use pytorch device: cuda\n",
            "INFO:mteb.evaluation.MTEB:\n",
            "\n",
            "## Evaluating 1 tasks:\n",
            "\u001b[30m───────────────────────────────────────── \u001b[0m\u001b[1mSelected tasks \u001b[0m\u001b[30m ──────────────────────────────────────────\u001b[0m\n",
            "\u001b[1mClassification\u001b[0m\n",
            "    - AmazonReviewsClassification, \u001b[3;90ms2s\u001b[0m, \u001b[3;31mmultilingual \u001b[0m\u001b[1;3;31m1\u001b[0m\u001b[3;31m \u001b[0m\u001b[3;31m/\u001b[0m\u001b[3;31m \u001b[0m\u001b[1;3;31m6\u001b[0m\u001b[3;31m langs\u001b[0m\n",
            "\n",
            "\n",
            "INFO:mteb.evaluation.MTEB:\n",
            "\n",
            "********************** Evaluating AmazonReviewsClassification **********************\n",
            "INFO:mteb.evaluation.MTEB:Loading dataset for AmazonReviewsClassification\n",
            "INFO:mteb.abstasks.AbsTaskClassification:\n",
            "Task: AmazonReviewsClassification, split: test, language: en. Running...\n",
            "INFO:mteb.abstasks.AbsTaskClassification:========== Experiment 1/10 ==========\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Encoding 40 training sentences...\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Encoding 5000 test sentences...\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Fitting logistic regression classifier...\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Evaluating...\n",
            "INFO:mteb.abstasks.AbsTaskClassification:========== Experiment 2/10 ==========\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Encoding 40 training sentences...\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Encoding 5000 test sentences...\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Fitting logistic regression classifier...\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Evaluating...\n",
            "INFO:mteb.abstasks.AbsTaskClassification:========== Experiment 3/10 ==========\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Encoding 40 training sentences...\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Encoding 5000 test sentences...\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Fitting logistic regression classifier...\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Evaluating...\n",
            "INFO:mteb.abstasks.AbsTaskClassification:========== Experiment 4/10 ==========\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Encoding 40 training sentences...\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Encoding 5000 test sentences...\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Fitting logistic regression classifier...\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Evaluating...\n",
            "INFO:mteb.abstasks.AbsTaskClassification:========== Experiment 5/10 ==========\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Encoding 40 training sentences...\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Encoding 5000 test sentences...\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Fitting logistic regression classifier...\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Evaluating...\n",
            "INFO:mteb.abstasks.AbsTaskClassification:========== Experiment 6/10 ==========\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Encoding 40 training sentences...\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Encoding 5000 test sentences...\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Fitting logistic regression classifier...\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Evaluating...\n",
            "INFO:mteb.abstasks.AbsTaskClassification:========== Experiment 7/10 ==========\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Encoding 40 training sentences...\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Encoding 5000 test sentences...\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Fitting logistic regression classifier...\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Evaluating...\n",
            "INFO:mteb.abstasks.AbsTaskClassification:========== Experiment 8/10 ==========\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Encoding 40 training sentences...\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Encoding 5000 test sentences...\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Fitting logistic regression classifier...\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Evaluating...\n",
            "INFO:mteb.abstasks.AbsTaskClassification:========== Experiment 9/10 ==========\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Encoding 40 training sentences...\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Encoding 5000 test sentences...\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Fitting logistic regression classifier...\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Evaluating...\n",
            "INFO:mteb.abstasks.AbsTaskClassification:========== Experiment 10/10 ==========\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Encoding 40 training sentences...\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Encoding 5000 test sentences...\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Fitting logistic regression classifier...\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Evaluating...\n",
            "INFO:mteb.evaluation.MTEB:Evaluation for AmazonReviewsClassification on test took 87.09 seconds\n",
            "INFO:mteb.evaluation.MTEB:Scores: {'en': {'accuracy': 0.46748, 'f1': 0.4464064623887645, 'accuracy_stderr': 0.010910618680899811, 'f1_stderr': 0.013427744962035876, 'main_score': 0.46748}, 'evaluation_time': 87.09}\n",
            "--DONE--\n",
            "\n",
            "\n",
            "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: hkunlp/instructor-large\n",
            "load INSTRUCTOR_Transformer\n",
            "max_seq_length  512\n",
            "INFO:sentence_transformers.SentenceTransformer:Use pytorch device: cuda\n",
            "INFO:mteb.evaluation.MTEB:\n",
            "\n",
            "## Evaluating 1 tasks:\n",
            "\u001b[30m───────────────────────────────────────── \u001b[0m\u001b[1mSelected tasks \u001b[0m\u001b[30m ──────────────────────────────────────────\u001b[0m\n",
            "\u001b[1mClassification\u001b[0m\n",
            "    - Banking77Classification, \u001b[3;90ms2s\u001b[0m\n",
            "\n",
            "\n",
            "INFO:mteb.evaluation.MTEB:\n",
            "\n",
            "********************** Evaluating Banking77Classification **********************\n",
            "INFO:mteb.evaluation.MTEB:Loading dataset for Banking77Classification\n",
            "INFO:mteb.abstasks.AbsTaskClassification:\n",
            "Task: Banking77Classification, split: test. Running...\n",
            "INFO:mteb.abstasks.AbsTaskClassification:========== Experiment 1/10 ==========\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Encoding 616 training sentences...\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Encoding 3080 test sentences...\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Fitting logistic regression classifier...\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Evaluating...\n",
            "INFO:mteb.abstasks.AbsTaskClassification:========== Experiment 2/10 ==========\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Encoding 616 training sentences...\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Encoding 3080 test sentences...\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Fitting logistic regression classifier...\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Evaluating...\n",
            "INFO:mteb.abstasks.AbsTaskClassification:========== Experiment 3/10 ==========\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Encoding 616 training sentences...\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Encoding 3080 test sentences...\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Fitting logistic regression classifier...\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Evaluating...\n",
            "INFO:mteb.abstasks.AbsTaskClassification:========== Experiment 4/10 ==========\n",
            "INFO:mteb.evaluation.evaluators.ClassificationEvaluator:Encoding 616 training sentences...\n"
          ]
        }
      ],
      "source": [
        "!python mteb_eval.py --model large --task retrieval"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python mteb_eval.py --model base --task retrieval"
      ],
      "metadata": {
        "id": "_MwyBlqoxKUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python mteb_eval.py --model xl --task retrieval"
      ],
      "metadata": {
        "id": "nVI2KeRvxNXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python mteb_eval.py --model base --task sts"
      ],
      "metadata": {
        "id": "wvvhWFmhxQTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python mteb_eval.py --model large --task sts"
      ],
      "metadata": {
        "id": "-vV4VAQfxTej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python mteb_eval.py --model xl --task sts"
      ],
      "metadata": {
        "id": "yIbIY1k9xXMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python mteb_eval.py --model base --task clustering\n",
        "!python mteb_eval.py --model large --task clustering\n",
        "!python mteb_eval.py --model xl --task clustering"
      ],
      "metadata": {
        "id": "DijGX5k5xf_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcFIEkmrYGR5"
      },
      "source": [
        "#### Move result to drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "He7iVyV8X0gr"
      },
      "source": [
        "### Billboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGfZGLI9X3nd"
      },
      "source": [
        "#### Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3sVIDVZX5VS"
      },
      "source": [
        "#### Run eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4oisaf3ZGm9"
      },
      "outputs": [],
      "source": [
        "billboard_task_list = ['mscoco', 'cnn summary', 'machine translation']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UQho55gZcns",
        "outputId": "4bb239d0-3939-4045-847e-7082889cc58b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/instructor-embedding/evaluation/text_evaluation/main.py:7: DeprecationWarning: Please use `pearsonr` from the `scipy.stats` namespace, the `scipy.stats.stats` namespace is deprecated.\n",
            "  from scipy.stats.stats import pearsonr\n",
            "add prompt\n",
            ".gitattributes: 100% 1.48k/1.48k [00:00<00:00, 7.52MB/s]\n",
            "1_Pooling/config.json: 100% 270/270 [00:00<00:00, 1.63MB/s]\n",
            "2_Dense/config.json: 100% 116/116 [00:00<00:00, 711kB/s]\n",
            "pytorch_model.bin: 100% 3.15M/3.15M [00:00<00:00, 48.3MB/s]\n",
            "README.md: 100% 66.3k/66.3k [00:00<00:00, 83.8MB/s]\n",
            "config.json: 100% 1.53k/1.53k [00:00<00:00, 10.1MB/s]\n",
            "config_sentence_transformers.json: 100% 122/122 [00:00<00:00, 811kB/s]\n",
            "pytorch_model.bin: 100% 1.34G/1.34G [00:07<00:00, 173MB/s]\n",
            "sentence_bert_config.json: 100% 53.0/53.0 [00:00<00:00, 253kB/s]\n",
            "special_tokens_map.json: 100% 2.20k/2.20k [00:00<00:00, 12.6MB/s]\n",
            "spiece.model: 100% 792k/792k [00:00<00:00, 224MB/s]\n",
            "tokenizer.json: 100% 2.42M/2.42M [00:00<00:00, 30.3MB/s]\n",
            "tokenizer_config.json: 100% 2.41k/2.41k [00:00<00:00, 9.41MB/s]\n",
            "modules.json: 100% 461/461 [00:00<00:00, 2.62MB/s]\n",
            "load INSTRUCTOR_Transformer\n",
            "max_seq_length  512\n",
            "PearsonRResult(statistic=0.4157232670260313, pvalue=4.5927614316588224e-105)\n",
            "\n",
            "\n",
            "/content/instructor-embedding/evaluation/text_evaluation/main.py:7: DeprecationWarning: Please use `pearsonr` from the `scipy.stats` namespace, the `scipy.stats.stats` namespace is deprecated.\n",
            "  from scipy.stats.stats import pearsonr\n",
            "add prompt\n",
            "load INSTRUCTOR_Transformer\n",
            "max_seq_length  512\n",
            "PearsonRResult(statistic=0.3033043222651611, pvalue=1.2948639887270706e-39)\n",
            "\n",
            "\n",
            "/content/instructor-embedding/evaluation/text_evaluation/main.py:7: DeprecationWarning: Please use `pearsonr` from the `scipy.stats` namespace, the `scipy.stats.stats` namespace is deprecated.\n",
            "  from scipy.stats.stats import pearsonr\n",
            "add prompt\n",
            "load INSTRUCTOR_Transformer\n",
            "max_seq_length  512\n",
            "PearsonRResult(statistic=0.388482226615209, pvalue=0.0)\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!python billboard.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python billboard.py --model xl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEB_LcQN0VPl",
        "outputId": "0a599efb-e804-469e-b074-3a15d38ab398"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/instructor-embedding/evaluation/text_evaluation/main.py:7: DeprecationWarning: Please use `pearsonr` from the `scipy.stats` namespace, the `scipy.stats.stats` namespace is deprecated.\n",
            "  from scipy.stats.stats import pearsonr\n",
            "add prompt\n",
            ".gitattributes: 100% 1.48k/1.48k [00:00<00:00, 7.80MB/s]\n",
            "1_Pooling/config.json: 100% 270/270 [00:00<00:00, 1.61MB/s]\n",
            "2_Dense/config.json: 100% 116/116 [00:00<00:00, 774kB/s]\n",
            "pytorch_model.bin: 100% 3.15M/3.15M [00:00<00:00, 35.2MB/s]\n",
            "README.md: 100% 66.3k/66.3k [00:00<00:00, 32.7MB/s]\n",
            "config.json: 100% 1.52k/1.52k [00:00<00:00, 9.68MB/s]\n",
            "config_sentence_transformers.json: 100% 122/122 [00:00<00:00, 776kB/s]\n",
            "pytorch_model.bin: 100% 4.96G/4.96G [00:48<00:00, 102MB/s] \n",
            "sentence_bert_config.json: 100% 53.0/53.0 [00:00<00:00, 259kB/s]\n",
            "special_tokens_map.json: 100% 2.20k/2.20k [00:00<00:00, 12.0MB/s]\n",
            "spiece.model: 100% 792k/792k [00:00<00:00, 252MB/s]\n",
            "tokenizer.json: 100% 2.42M/2.42M [00:00<00:00, 31.5MB/s]\n",
            "tokenizer_config.json: 100% 2.40k/2.40k [00:00<00:00, 14.0MB/s]\n",
            "modules.json: 100% 461/461 [00:00<00:00, 2.81MB/s]\n",
            "load INSTRUCTOR_Transformer\n",
            "max_seq_length  512\n",
            "PearsonRResult(statistic=0.39554851872051533, pvalue=2.0504327157238082e-94)\n",
            "\n",
            "\n",
            "/content/instructor-embedding/evaluation/text_evaluation/main.py:7: DeprecationWarning: Please use `pearsonr` from the `scipy.stats` namespace, the `scipy.stats.stats` namespace is deprecated.\n",
            "  from scipy.stats.stats import pearsonr\n",
            "add prompt\n",
            "load INSTRUCTOR_Transformer\n",
            "max_seq_length  512\n",
            "PearsonRResult(statistic=0.31735037879729977, pvalue=2.119127498485315e-43)\n",
            "\n",
            "\n",
            "/content/instructor-embedding/evaluation/text_evaluation/main.py:7: DeprecationWarning: Please use `pearsonr` from the `scipy.stats` namespace, the `scipy.stats.stats` namespace is deprecated.\n",
            "  from scipy.stats.stats import pearsonr\n",
            "add prompt\n",
            "load INSTRUCTOR_Transformer\n",
            "max_seq_length  512\n",
            "PearsonRResult(statistic=0.3054921208892365, pvalue=0.0)\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python billboard.py --model base"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9njgTVwp1ZKD",
        "outputId": "c7d7cf4c-4554-4774-a541-9f6ea5edc1fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/instructor-embedding/evaluation/text_evaluation/main.py:7: DeprecationWarning: Please use `pearsonr` from the `scipy.stats` namespace, the `scipy.stats.stats` namespace is deprecated.\n",
            "  from scipy.stats.stats import pearsonr\n",
            "add prompt\n",
            ".gitattributes: 100% 1.48k/1.48k [00:00<00:00, 7.66MB/s]\n",
            "1_Pooling/config.json: 100% 270/270 [00:00<00:00, 1.39MB/s]\n",
            "2_Dense/config.json: 100% 115/115 [00:00<00:00, 736kB/s]\n",
            "pytorch_model.bin: 100% 2.36M/2.36M [00:00<00:00, 15.6MB/s]\n",
            "README.md: 100% 66.2k/66.2k [00:00<00:00, 1.01MB/s]\n",
            "config.json: 100% 1.55k/1.55k [00:00<00:00, 9.27MB/s]\n",
            "config_sentence_transformers.json: 100% 122/122 [00:00<00:00, 729kB/s]\n",
            "pytorch_model.bin: 100% 439M/439M [00:11<00:00, 37.8MB/s]\n",
            "sentence_bert_config.json: 100% 53.0/53.0 [00:00<00:00, 309kB/s]\n",
            "special_tokens_map.json: 100% 2.20k/2.20k [00:00<00:00, 10.7MB/s]\n",
            "spiece.model: 100% 792k/792k [00:00<00:00, 7.38MB/s]\n",
            "tokenizer.json: 100% 2.42M/2.42M [00:00<00:00, 11.9MB/s]\n",
            "tokenizer_config.json: 100% 2.43k/2.43k [00:00<00:00, 12.6MB/s]\n",
            "modules.json: 100% 461/461 [00:00<00:00, 2.55MB/s]\n",
            "load INSTRUCTOR_Transformer\n",
            "max_seq_length  512\n",
            "PearsonRResult(statistic=0.41188767438030355, pvalue=5.543070678807008e-103)\n",
            "\n",
            "\n",
            "/content/instructor-embedding/evaluation/text_evaluation/main.py:7: DeprecationWarning: Please use `pearsonr` from the `scipy.stats` namespace, the `scipy.stats.stats` namespace is deprecated.\n",
            "  from scipy.stats.stats import pearsonr\n",
            "add prompt\n",
            "load INSTRUCTOR_Transformer\n",
            "max_seq_length  512\n",
            "PearsonRResult(statistic=0.23862137534421463, pvalue=1.008448137022683e-24)\n",
            "\n",
            "\n",
            "/content/instructor-embedding/evaluation/text_evaluation/main.py:7: DeprecationWarning: Please use `pearsonr` from the `scipy.stats` namespace, the `scipy.stats.stats` namespace is deprecated.\n",
            "  from scipy.stats.stats import pearsonr\n",
            "add prompt\n",
            "load INSTRUCTOR_Transformer\n",
            "max_seq_length  512\n",
            "PearsonRResult(statistic=0.382093540944323, pvalue=0.0)\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFE-I5d4YQB7"
      },
      "source": [
        "#### Move result to drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFTAN0r0X70m"
      },
      "source": [
        "### Promt Retrieval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSUvuNsyX_J1"
      },
      "source": [
        "#### Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hor_p7-hYCON"
      },
      "source": [
        "#### Run eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zP0uGXCWZ1LF"
      },
      "outputs": [],
      "source": [
        "promt_retrieval_task_list = ['rte','sst5','mrpc','dbpedia_14','hellaswag']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://huggingface.co/EleutherAI/gpt-j-6b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVwNFHpkBEHv",
        "outputId": "57c066c8-c220-40e9-fc24-10ed79704312"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'gpt-j-6b'...\n",
            "remote: Enumerating objects: 109, done.\u001b[K\n",
            "remote: Total 109 (delta 0), reused 0 (delta 0), pack-reused 109\u001b[K\n",
            "Receiving objects: 100% (109/109), 1.40 MiB | 9.51 MiB/s, done.\n",
            "Resolving deltas: 100% (53/53), done.\n",
            "Downloading flax_model.msgpack (24 GB)\n",
            "Error downloading object: flax_model.msgpack (aa31d9d): Smudge error: Error downloading flax_model.msgpack (aa31d9d4a2187732f82a98705e9ec24ec424335e323420affd8b9c5cd7a73b17): cannot write data to tempfile \"/content/instructor-embedding/gpt-j-6b/.git/lfs/incomplete/aa31d9d4a2187732f82a98705e9ec24ec424335e323420affd8b9c5cd7a73b17909518265\": write /content/instructor-embedding/gpt-j-6b/.git/lfs/incomplete/aa31d9d4a2187732f82a98705e9ec24ec424335e323420affd8b9c5cd7a73b17909518265: no space left on device\n",
            "Unable to log panic to /content/instructor-embedding/gpt-j-6b/.git/lfs/logs: mkdir /content/instructor-embedding/gpt-j-6b/.git/lfs/logs: no space left on device\n",
            "\n",
            "git-lfs/3.0.2 (GitHub; linux amd64; go 1.18.1)\n",
            "git version 2.34.1\n",
            "\n",
            "$ git-lfs filter-process\n",
            "Error downloading object: flax_model.msgpack (aa31d9d): Smudge error: Error downloading flax_model.msgpack (aa31d9d4a2187732f82a98705e9ec24ec424335e323420affd8b9c5cd7a73b17): cannot write data to tempfile \"/content/instructor-embedding/gpt-j-6b/.git/lfs/incomplete/aa31d9d4a2187732f82a98705e9ec24ec424335e323420affd8b9c5cd7a73b17909518265\": write /content/instructor-embedding/gpt-j-6b/.git/lfs/incomplete/aa31d9d4a2187732f82a98705e9ec24ec424335e323420affd8b9c5cd7a73b17909518265: no space left on device\n",
            "\n",
            "write /content/instructor-embedding/gpt-j-6b/.git/lfs/incomplete/aa31d9d4a2187732f82a98705e9ec24ec424335e323420affd8b9c5cd7a73b17909518265: no space left on device\n",
            "cannot write data to tempfile \"/content/instructor-embedding/gpt-j-6b/.git/lfs/incomplete/aa31d9d4a2187732f82a98705e9ec24ec424335e323420affd8b9c5cd7a73b17909518265\"\n",
            "github.com/git-lfs/git-lfs/errors.newWrappedError\n",
            "\tgithub.com/git-lfs/git-lfs/errors/types.go:225\n",
            "github.com/git-lfs/git-lfs/errors.Wrapf\n",
            "\tgithub.com/git-lfs/git-lfs/errors/errors.go:85\n",
            "github.com/git-lfs/git-lfs/tq.(*basicDownloadAdapter).download\n",
            "\tgithub.com/git-lfs/git-lfs/tq/basic_download.go:244\n",
            "github.com/git-lfs/git-lfs/tq.(*basicDownloadAdapter).DoTransfer\n",
            "\tgithub.com/git-lfs/git-lfs/tq/basic_download.go:95\n",
            "github.com/git-lfs/git-lfs/tq.(*adapterBase).worker\n",
            "\tgithub.com/git-lfs/git-lfs/tq/adapterbase.go:182\n",
            "runtime.goexit\n",
            "\truntime/asm_amd64.s:1571\n",
            "Error downloading flax_model.msgpack (aa31d9d4a2187732f82a98705e9ec24ec424335e323420affd8b9c5cd7a73b17)\n",
            "github.com/git-lfs/git-lfs/errors.newWrappedError\n",
            "\tgithub.com/git-lfs/git-lfs/errors/types.go:225\n",
            "github.com/git-lfs/git-lfs/errors.Wrapf\n",
            "\tgithub.com/git-lfs/git-lfs/errors/errors.go:85\n",
            "github.com/git-lfs/git-lfs/lfs.(*GitFilter).downloadFile\n",
            "\tgithub.com/git-lfs/git-lfs/lfs/gitfilter_smudge.go:119\n",
            "github.com/git-lfs/git-lfs/lfs.(*GitFilter).Smudge\n",
            "\tgithub.com/git-lfs/git-lfs/lfs/gitfilter_smudge.go:78\n",
            "github.com/git-lfs/git-lfs/commands.smudge\n",
            "\tgithub.com/git-lfs/git-lfs/commands/command_smudge.go:127\n",
            "github.com/git-lfs/git-lfs/commands.filterCommand\n",
            "\tgithub.com/git-lfs/git-lfs/commands/command_filter_process.go:122\n",
            "github.com/spf13/cobra.(*Command).execute\n",
            "\tgithub.com/spf13/cobra/command.go:860\n",
            "github.com/spf13/cobra.(*Command).ExecuteC\n",
            "\tgithub.com/spf13/cobra/command.go:974\n",
            "github.com/spf13/cobra.(*Command).Execute\n",
            "\tgithub.com/spf13/cobra/command.go:902\n",
            "github.com/git-lfs/git-lfs/commands.Run\n",
            "\tgithub.com/git-lfs/git-lfs/commands/run.go:105\n",
            "main.main\n",
            "\tgithub.com/git-lfs/git-lfs/git-lfs.go:33\n",
            "runtime.main\n",
            "\truntime/proc.go:250\n",
            "runtime.goexit\n",
            "\truntime/asm_amd64.s:1571\n",
            "Smudge error\n",
            "github.com/git-lfs/git-lfs/errors.newWrappedError\n",
            "\tgithub.com/git-lfs/git-lfs/errors/types.go:225\n",
            "github.com/git-lfs/git-lfs/errors.NewSmudgeError\n",
            "\tgithub.com/git-lfs/git-lfs/errors/types.go:311\n",
            "github.com/git-lfs/git-lfs/lfs.(*GitFilter).Smudge\n",
            "\tgithub.com/git-lfs/git-lfs/lfs/gitfilter_smudge.go:87\n",
            "github.com/git-lfs/git-lfs/commands.smudge\n",
            "\tgithub.com/git-lfs/git-lfs/commands/command_smudge.go:127\n",
            "github.com/git-lfs/git-lfs/commands.filterCommand\n",
            "\tgithub.com/git-lfs/git-lfs/commands/command_filter_process.go:122\n",
            "github.com/spf13/cobra.(*Command).execute\n",
            "\tgithub.com/spf13/cobra/command.go:860\n",
            "github.com/spf13/cobra.(*Command).ExecuteC\n",
            "\tgithub.com/spf13/cobra/command.go:974\n",
            "github.com/spf13/cobra.(*Command).Execute\n",
            "\tgithub.com/spf13/cobra/command.go:902\n",
            "github.com/git-lfs/git-lfs/commands.Run\n",
            "\tgithub.com/git-lfs/git-lfs/commands/run.go:105\n",
            "main.main\n",
            "\tgithub.com/git-lfs/git-lfs/git-lfs.go:33\n",
            "runtime.main\n",
            "\truntime/proc.go:250\n",
            "runtime.goexit\n",
            "\truntime/asm_amd64.s:1571\n",
            "\n",
            "Current time in UTC: \n",
            "2023-12-22 15:02:03\n",
            "\n",
            "ENV:\n",
            "LocalWorkingDir=/content/instructor-embedding/gpt-j-6b\n",
            "LocalGitDir=/content/instructor-embedding/gpt-j-6b/.git\n",
            "LocalGitStorageDir=/content/instructor-embedding/gpt-j-6b/.git\n",
            "LocalMediaDir=/content/instructor-embedding/gpt-j-6b/.git/lfs/objects\n",
            "LocalReferenceDirs=\n",
            "TempDir=/content/instructor-embedding/gpt-j-6b/.git/lfs/tmp\n",
            "ConcurrentTransfers=8\n",
            "TusTransfers=false\n",
            "BasicTransfersOnly=false\n",
            "SkipDownloadErrors=false\n",
            "FetchRecentAlways=false\n",
            "FetchRecentRefsDays=7\n",
            "FetchRecentCommitsDays=0\n",
            "FetchRecentRefsIncludeRemotes=true\n",
            "PruneOffsetDays=3\n",
            "PruneVerifyRemoteAlways=false\n",
            "PruneRemoteName=origin\n",
            "LfsStorageDir=/content/instructor-embedding/gpt-j-6b/.git/lfs\n",
            "AccessDownload=none\n",
            "AccessUpload=none\n",
            "DownloadTransfers=basic,lfs-standalone-file,ssh\n",
            "UploadTransfers=basic,lfs-standalone-file,ssh\n",
            "GIT_EXEC_PATH=/usr/lib/git-core\n",
            "GIT_DIR=/content/instructor-embedding/gpt-j-6b/.git\n",
            "GIT_PAGER=cat\n",
            "\n",
            "Client IP addresses:\n",
            "172.28.0.12\n",
            "error: external filter 'git-lfs filter-process' failed\n",
            "fatal: flax_model.msgpack: smudge filter lfs failed\n",
            "warning: Clone succeeded, but checkout failed.\n",
            "You can inspect what was checked out with 'git status'\n",
            "and retry with 'git restore --source=HEAD :/'\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sypZbNLhaDcT",
        "outputId": "3c396722-28cc-47ee-dcdc-194de0913cc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/instructor-embedding/evaluation/prompt_retrieval/main.py\", line 4, in <module>\n",
            "    import torch\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 1442, in <module>\n",
            "    import torch.utils.data\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/__init__.py\", line 21, in <module>\n",
            "    from torch.utils.data.datapipes.datapipe import (\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/__init__.py\", line 1, in <module>\n",
            "    from . import iter\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/iter/__init__.py\", line 1, in <module>\n",
            "    from torch.utils.data.datapipes.iter.utils import (\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/iter/utils.py\", line 3, in <module>\n",
            "    from torch.utils.data.datapipes.datapipe import IterDataPipe\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/datapipe.py\", line 7, in <module>\n",
            "    from torch.utils.data.datapipes.utils.common import (\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/utils/common.py\", line 13, in <module>\n",
            "    from torch.utils.data._utils.serialization import DILL_AVAILABLE\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/serialization.py\", line 2, in <module>\n",
            "    import dill\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dill/__init__.py\", line 33, in <module>\n",
            "    from .session import (\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dill/session.py\", line 36, in <module>\n",
            "    TEMPDIR = pathlib.PurePath(tempfile.gettempdir())\n",
            "  File \"/usr/lib/python3.10/tempfile.py\", line 438, in gettempdir\n",
            "    return _os.fsdecode(_gettempdir())\n",
            "  File \"/usr/lib/python3.10/tempfile.py\", line 431, in _gettempdir\n",
            "    tempdir = _get_default_tempdir()\n",
            "  File \"/usr/lib/python3.10/tempfile.py\", line 362, in _get_default_tempdir\n",
            "    raise FileNotFoundError(_errno.ENOENT,\n",
            "FileNotFoundError: [Errno 2] No usable temporary directory found in ['/tmp', '/var/tmp', '/usr/tmp', '/content/instructor-embedding/evaluation/prompt_retrieval']\n",
            "\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/instructor-embedding/evaluation/prompt_retrieval/main.py\", line 4, in <module>\n",
            "    import torch\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 1442, in <module>\n",
            "    import torch.utils.data\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/__init__.py\", line 21, in <module>\n",
            "    from torch.utils.data.datapipes.datapipe import (\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/__init__.py\", line 1, in <module>\n",
            "    from . import iter\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/iter/__init__.py\", line 1, in <module>\n",
            "    from torch.utils.data.datapipes.iter.utils import (\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/iter/utils.py\", line 3, in <module>\n",
            "    from torch.utils.data.datapipes.datapipe import IterDataPipe\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/datapipe.py\", line 7, in <module>\n",
            "    from torch.utils.data.datapipes.utils.common import (\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/utils/common.py\", line 13, in <module>\n",
            "    from torch.utils.data._utils.serialization import DILL_AVAILABLE\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/serialization.py\", line 2, in <module>\n",
            "    import dill\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dill/__init__.py\", line 33, in <module>\n",
            "    from .session import (\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dill/session.py\", line 36, in <module>\n",
            "    TEMPDIR = pathlib.PurePath(tempfile.gettempdir())\n",
            "  File \"/usr/lib/python3.10/tempfile.py\", line 438, in gettempdir\n",
            "    return _os.fsdecode(_gettempdir())\n",
            "  File \"/usr/lib/python3.10/tempfile.py\", line 431, in _gettempdir\n",
            "    tempdir = _get_default_tempdir()\n",
            "  File \"/usr/lib/python3.10/tempfile.py\", line 362, in _get_default_tempdir\n",
            "    raise FileNotFoundError(_errno.ENOENT,\n",
            "FileNotFoundError: [Errno 2] No usable temporary directory found in ['/tmp', '/var/tmp', '/usr/tmp', '/content/instructor-embedding/evaluation/prompt_retrieval']\n",
            "\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/instructor-embedding/evaluation/prompt_retrieval/main.py\", line 4, in <module>\n",
            "    import torch\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 1442, in <module>\n",
            "    import torch.utils.data\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/__init__.py\", line 21, in <module>\n",
            "    from torch.utils.data.datapipes.datapipe import (\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/__init__.py\", line 1, in <module>\n",
            "    from . import iter\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/iter/__init__.py\", line 1, in <module>\n",
            "    from torch.utils.data.datapipes.iter.utils import (\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/iter/utils.py\", line 3, in <module>\n",
            "    from torch.utils.data.datapipes.datapipe import IterDataPipe\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/datapipe.py\", line 7, in <module>\n",
            "    from torch.utils.data.datapipes.utils.common import (\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/utils/common.py\", line 13, in <module>\n",
            "    from torch.utils.data._utils.serialization import DILL_AVAILABLE\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/serialization.py\", line 2, in <module>\n",
            "    import dill\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dill/__init__.py\", line 33, in <module>\n",
            "    from .session import (\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dill/session.py\", line 36, in <module>\n",
            "    TEMPDIR = pathlib.PurePath(tempfile.gettempdir())\n",
            "  File \"/usr/lib/python3.10/tempfile.py\", line 438, in gettempdir\n",
            "    return _os.fsdecode(_gettempdir())\n",
            "  File \"/usr/lib/python3.10/tempfile.py\", line 431, in _gettempdir\n",
            "    tempdir = _get_default_tempdir()\n",
            "  File \"/usr/lib/python3.10/tempfile.py\", line 362, in _get_default_tempdir\n",
            "    raise FileNotFoundError(_errno.ENOENT,\n",
            "FileNotFoundError: [Errno 2] No usable temporary directory found in ['/tmp', '/var/tmp', '/usr/tmp', '/content/instructor-embedding/evaluation/prompt_retrieval']\n",
            "\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/instructor-embedding/evaluation/prompt_retrieval/main.py\", line 4, in <module>\n",
            "    import torch\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 1442, in <module>\n",
            "    import torch.utils.data\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/__init__.py\", line 21, in <module>\n",
            "    from torch.utils.data.datapipes.datapipe import (\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/__init__.py\", line 1, in <module>\n",
            "    from . import iter\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/iter/__init__.py\", line 1, in <module>\n",
            "    from torch.utils.data.datapipes.iter.utils import (\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/iter/utils.py\", line 3, in <module>\n",
            "    from torch.utils.data.datapipes.datapipe import IterDataPipe\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/datapipe.py\", line 7, in <module>\n",
            "    from torch.utils.data.datapipes.utils.common import (\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/utils/common.py\", line 13, in <module>\n",
            "    from torch.utils.data._utils.serialization import DILL_AVAILABLE\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/serialization.py\", line 2, in <module>\n",
            "    import dill\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dill/__init__.py\", line 33, in <module>\n",
            "    from .session import (\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dill/session.py\", line 36, in <module>\n",
            "    TEMPDIR = pathlib.PurePath(tempfile.gettempdir())\n",
            "  File \"/usr/lib/python3.10/tempfile.py\", line 438, in gettempdir\n",
            "    return _os.fsdecode(_gettempdir())\n",
            "  File \"/usr/lib/python3.10/tempfile.py\", line 431, in _gettempdir\n",
            "    tempdir = _get_default_tempdir()\n",
            "  File \"/usr/lib/python3.10/tempfile.py\", line 362, in _get_default_tempdir\n",
            "    raise FileNotFoundError(_errno.ENOENT,\n",
            "FileNotFoundError: [Errno 2] No usable temporary directory found in ['/tmp', '/var/tmp', '/usr/tmp', '/content/instructor-embedding/evaluation/prompt_retrieval']\n",
            "\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/instructor-embedding/evaluation/prompt_retrieval/main.py\", line 4, in <module>\n",
            "    import torch\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 1442, in <module>\n",
            "    import torch.utils.data\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/__init__.py\", line 21, in <module>\n",
            "    from torch.utils.data.datapipes.datapipe import (\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/__init__.py\", line 1, in <module>\n",
            "    from . import iter\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/iter/__init__.py\", line 1, in <module>\n",
            "    from torch.utils.data.datapipes.iter.utils import (\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/iter/utils.py\", line 3, in <module>\n",
            "    from torch.utils.data.datapipes.datapipe import IterDataPipe\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/datapipe.py\", line 7, in <module>\n",
            "    from torch.utils.data.datapipes.utils.common import (\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/utils/common.py\", line 13, in <module>\n",
            "    from torch.utils.data._utils.serialization import DILL_AVAILABLE\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/serialization.py\", line 2, in <module>\n",
            "    import dill\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dill/__init__.py\", line 33, in <module>\n",
            "    from .session import (\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dill/session.py\", line 36, in <module>\n",
            "    TEMPDIR = pathlib.PurePath(tempfile.gettempdir())\n",
            "  File \"/usr/lib/python3.10/tempfile.py\", line 438, in gettempdir\n",
            "    return _os.fsdecode(_gettempdir())\n",
            "  File \"/usr/lib/python3.10/tempfile.py\", line 431, in _gettempdir\n",
            "    tempdir = _get_default_tempdir()\n",
            "  File \"/usr/lib/python3.10/tempfile.py\", line 362, in _get_default_tempdir\n",
            "    raise FileNotFoundError(_errno.ENOENT,\n",
            "FileNotFoundError: [Errno 2] No usable temporary directory found in ['/tmp', '/var/tmp', '/usr/tmp', '/content/instructor-embedding/evaluation/prompt_retrieval']\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!python promt_retrieval.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5fubrC3MbP-",
        "outputId": "2f1eaa56-b4d8-44da-d74b-fed435097736"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-12-20 17:09:11.632410: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-20 17:09:11.632464: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-20 17:09:11.633969: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-20 17:09:11.641861: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-12-20 17:09:12.979492: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Downloading builder script: 100% 28.8k/28.8k [00:00<00:00, 34.3MB/s]\n",
            "Downloading metadata: 100% 28.7k/28.7k [00:00<00:00, 44.9MB/s]\n",
            "Downloading readme: 100% 27.9k/27.9k [00:00<00:00, 46.4MB/s]\n",
            "Downloading data: 100% 697k/697k [00:00<00:00, 14.6MB/s]\n",
            "Generating train split: 100% 2490/2490 [00:00<00:00, 17632.17 examples/s]\n",
            "Generating validation split: 100% 277/277 [00:00<00:00, 18248.99 examples/s]\n",
            "Generating test split: 100% 3000/3000 [00:00<00:00, 19719.71 examples/s]\n",
            "process rte examples: 100% 2490/2490 [00:00<00:00, 1193442.69it/s]\n",
            "process rte examples: 100% 256/256 [00:00<00:00, 823421.64it/s]\n",
            ".gitattributes: 100% 1.48k/1.48k [00:00<00:00, 8.24MB/s]\n",
            "1_Pooling/config.json: 100% 270/270 [00:00<00:00, 1.03MB/s]\n",
            "2_Dense/config.json: 100% 116/116 [00:00<00:00, 575kB/s]\n",
            "pytorch_model.bin: 100% 3.15M/3.15M [00:00<00:00, 64.0MB/s]\n",
            "README.md: 100% 66.3k/66.3k [00:00<00:00, 52.9MB/s]\n",
            "config.json: 100% 1.53k/1.53k [00:00<00:00, 8.93MB/s]\n",
            "config_sentence_transformers.json: 100% 122/122 [00:00<00:00, 699kB/s]\n",
            "pytorch_model.bin: 100% 1.34G/1.34G [00:11<00:00, 120MB/s]\n",
            "sentence_bert_config.json: 100% 53.0/53.0 [00:00<00:00, 304kB/s]\n",
            "special_tokens_map.json: 100% 2.20k/2.20k [00:00<00:00, 9.66MB/s]\n",
            "spiece.model: 100% 792k/792k [00:00<00:00, 313MB/s]\n",
            "tokenizer.json: 100% 2.42M/2.42M [00:00<00:00, 6.20MB/s]\n",
            "tokenizer_config.json: 100% 2.41k/2.41k [00:00<00:00, 13.2MB/s]\n",
            "modules.json: 100% 461/461 [00:00<00:00, 2.77MB/s]\n",
            "load INSTRUCTOR_Transformer\n",
            "max_seq_length  512\n",
            "calculate embeddings:   0% 0/125 [00:00<?, ?it/s]add prompt\n",
            "calculate embeddings: 100% 125/125 [01:33<00:00,  1.34it/s]\n",
            "load INSTRUCTOR_Transformer\n",
            "max_seq_length  512\n",
            "calculate embeddings:   0% 0/13 [00:00<?, ?it/s]add prompt\n",
            "calculate embeddings: 100% 13/13 [00:10<00:00,  1.20it/s]\n",
            "Downloading: 100% 665/665 [00:00<00:00, 3.00MB/s]\n",
            "Downloading: 100% 0.99M/0.99M [00:00<00:00, 5.19MB/s]\n",
            "Downloading: 100% 446k/446k [00:00<00:00, 6.12MB/s]\n",
            "Downloading: 100% 1.29M/1.29M [00:00<00:00, 6.68MB/s]\n",
            "Logging from MetaICLModel:\t Setting up for local_rank=-1, world_size=1\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\", line 601, in _get_config_dict\n",
            "    resolved_config_file = cached_path(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\", line 284, in cached_path\n",
            "    output_path = get_from_cache(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\", line 501, in get_from_cache\n",
            "    raise OSError(\n",
            "OSError: Distant resource does not have an ETag, we won't be able to reliably ensure reproducibility.\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/instructor-embedding/evaluation/prompt_retrieval/main.py\", line 62, in <module>\n",
            "    inference_model.load()\n",
            "  File \"/content/instructor-embedding/evaluation/prompt_retrieval/MetaICL/metaicl/model.py\", line 84, in load\n",
            "    model = AutoModelForCausalLM.from_pretrained(model_name,cache_dir=self.args.model_cache_dir)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\", line 423, in from_pretrained\n",
            "    config, kwargs = AutoConfig.from_pretrained(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/configuration_auto.py\", line 705, in from_pretrained\n",
            "    config_dict, _ = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\", line 553, in get_config_dict\n",
            "    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\", line 641, in _get_config_dict\n",
            "    raise EnvironmentError(\n",
            "OSError: Can't load config for 'EleutherAI/gpt-j-6B'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'EleutherAI/gpt-j-6B' is the correct path to a directory containing a config.json file\n"
          ]
        }
      ],
      "source": [
        "!./eval_promt_retrieval.sh large rte"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wx9nCxDrYS0Y"
      },
      "source": [
        "#### Move result to drive"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "7YFzOWRaHAsA"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}